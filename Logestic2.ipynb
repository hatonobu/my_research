{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './data'\n",
      "/mnt/data1/home/yoshida/my_research/data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import gensim\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pymagnitude import Magnitude, MagnitudeUtils\n",
    "\n",
    "%cd ./data\n",
    "from func1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(\"oversample_text.csv\",encoding=\"utf-8\",engine=\"python\")  # データの読み込み\n",
    "#df_clean = makeclean_csv(\"oversample_text\")\n",
    "\n",
    "# ラベルと文章を分ける\n",
    "sentences = df.text.values\n",
    "labels = df.tag.values\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mecabによる形態素解析(ipadic-neologd),taggerを変えれば何でもok\n",
    "def tokenize(text):\n",
    "    cmd = 'echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n",
    "    m = MeCab.Tagger(\"-d {0}\".format(path))  #mecab-ipadic-neologd\n",
    "    #m = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
    "    node = m.parse(text)\n",
    "    node = node.split(\"\\n\")\n",
    "    text_tokenize = []\n",
    "    for i in range(len(node)):\n",
    "        word = node[i].split(\"\\t\")\n",
    "        if word[0] == \"EOS\":\n",
    "            break\n",
    "        text_tokenize.append(word[0])\n",
    "    \n",
    "    return text_tokenize\n",
    "\n",
    "tokenized_sentence = []\n",
    "for sentence in sentences:\n",
    "    sen = tokenize(sentence)\n",
    "    tokenized_sentence.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.KeyedVectors.load(\"./chive-1.1-mc90_gensim/chive-1.1-mc90.kv\")\n",
    "model = Magnitude(\"https://sudachi.s3-ap-northeast-1.amazonaws.com/chive/chive-1.1-mc90-aunit.magnitude\")\n",
    "#model.query(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = word2vec.Word2Vec(sentences=tokenized_sentence, vector_size=300, window=5, min_count=1, epochs=10)\n",
    "#model.wx[\"word\"]\n",
    "#model.save(\"./output.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = tokenize(sentence)\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\") # 特徴ベクトルの入れ物を初期化\n",
    "    for word in words:\n",
    "        feature_vec = np.add(feature_vec, model.query(word))\n",
    "    if len(words) > 0:\n",
    "        feature_vec = np.divide(feature_vec, len(words))\n",
    "    return feature_vec\n",
    "\n",
    "feature_vec_lis = []\n",
    "for i,sentence in enumerate(sentences):\n",
    "    feature_vec = avg_feature_vector(sentence,model,300)\n",
    "    feature_vec_lis.append(feature_vec)\n",
    "    \n",
    "#print(feature_vec_lis[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトル化したものを圧縮し、数次元のデータにする\n",
    "#主成分分析により２次元に圧縮する\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(feature_vec_lis)\n",
    "data_pca= pca.transform(feature_vec_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_3d = pd.DataFrame(np.concatenate([data_pca, y], axis=1), columns=['first component', 'second component','third component' 'tags']).astype({'first component':float, 'second component':float,'third component':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Predicted safety  Predicted flaming  Predicted spam\n",
      "Actual safety                 87                 61              65\n",
      "Actual flaming                40                 88              25\n",
      "Actual spam                    3                  2              29\n",
      "Accuracy : 0.51\n",
      "Precision: [0.66923077 0.58278146 0.24369748]\n",
      "Recall : [0.4084507  0.5751634  0.85294118]\n",
      "F1-Score: [0.50728863 0.57894737 0.37908497]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.41      0.51       213\n",
      "         1.0       0.58      0.58      0.58       153\n",
      "         2.0       0.24      0.85      0.38        34\n",
      "\n",
      "    accuracy                           0.51       400\n",
      "   macro avg       0.50      0.61      0.49       400\n",
      "weighted avg       0.60      0.51      0.52       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3値分類におけるロジスティック回帰\n",
    "# 7:3に学習データとテストデータを分割する\n",
    "X_train = data_pca\n",
    "y_train = labels\n",
    "df2 = pd.read_csv(\"test_data.csv\",encoding=\"utf-8\",engine=\"python\")  # データの読み込み\n",
    "test_sentences = df2.text.values\n",
    "y_test = df2.tag.values\n",
    "\n",
    "#testデータに対してベクトル化処理\n",
    "feature_vec_lis = []\n",
    "for i,sentence in enumerate(test_sentences):\n",
    "    feature_vec = avg_feature_vector(sentence,model,300)\n",
    "    feature_vec_lis.append(feature_vec)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(feature_vec_lis)\n",
    "X_test = pca.transform(feature_vec_lis)\n",
    "\n",
    "lr = LogisticRegression(random_state=SEED, n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "df_sub = pd.DataFrame(cm,columns=['Predicted safety', 'Predicted flaming','Predicted spam'], index=['Actual safety', 'Actual flaming','Actual spam'])\n",
    "print(df_sub)\n",
    "print(\"Accuracy : {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test,y_pred,average = None)))\n",
    "print(\"Recall : {}\".format(recall_score(y_test,y_pred, average = None)))\n",
    "print(\"F1-Score: {}\".format(f1_score(y_test,y_pred,average = None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
