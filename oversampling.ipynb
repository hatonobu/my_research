{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnetを利用したオーバーサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data1/home/yoshida/my_research/data\n"
     ]
    }
   ],
   "source": [
    "%cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -O wnjpn.db.gz http://compling.hss.ntu.edu.sg/wnja/data/1.1/wnjpn.db.gz\n",
    "#!gzip -d wnjpn.db.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordnet_jp as jp\n",
    "import pprint \n",
    "import sqlite3\n",
    "import subprocess\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"./wnjpn.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = '犬'\n",
    "synonym = jp.getSynonym(word) \n",
    "#wordlis = wordnet_jp.getWordsFromSenses(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_get(word):\n",
    "    synonym = {}\n",
    "    words = jp.getWords(word)\n",
    "    if words:\n",
    "        for w in words:\n",
    "            sense = jp.getSenses(w)\n",
    "            s = jp.getWordsFromSenses(sense)\n",
    "            synonym = dict(list(synonym.items()) + list(s.items()))\n",
    "\n",
    "    words_lis = []\n",
    "    lis = synonym.keys()\n",
    "    for name in lis:\n",
    "        for w in synonym[name]:\n",
    "             words_lis.append(w)\n",
    "    words_lis = set(words_lis)\n",
    "    return words_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mecabによる形態素解析(ipadic-neologd),taggerを変えれば何でもok\n",
    "def tokenize(text):\n",
    "    cmd = 'echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,shell=True).communicate()[0]).decode('utf-8')\n",
    "    m = MeCab.Tagger(\"-d {0}\".format(path))  #mecab-ipadic-neologd\n",
    "    #m = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n",
    "    node = m.parse(text)\n",
    "    node = node.split(\"\\n\")\n",
    "    noun = []\n",
    "    for i in range(len(node)):\n",
    "        word = node[i].split(\"\\t\")\n",
    "        if word[0] == \"EOS\":\n",
    "            break\n",
    "        #品詞に関する分類作業\n",
    "        word2 = word[1].split(\",\")\n",
    "        #print(word2)\n",
    "        if word2[0] == \"名詞\" or word2[0] == \"形容詞\":\n",
    "            noun.append(word[0])\n",
    "    \n",
    "    return noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['牡蠣', '美味しい', 'お土産', '何', '個']\n"
     ]
    }
   ],
   "source": [
    "#一つのテキストに対する実行例の表示\n",
    "#処理手順：名詞・形容詞の抽出→抽出した語句がWordnetにあるか→元の文章から該当箇所の変更\n",
    "text = \"この牡蠣とても美味しいからお土産に何個も買った。\"\n",
    "replace_sens = []\n",
    "noun_lis = tokenize(text)\n",
    "print(noun_lis)\n",
    "for n in noun_lis:\n",
    "    nouns = wordnet_get(n)\n",
    "    for noun in nouns:\n",
    "        sample = text.replace(n,noun)\n",
    "        replace_sens.append(sample)\n",
    "\n",
    "#文章をリスト化したものに対してすべて処理する関数\n",
    "def replace_sens_make(sens,num):\n",
    "    replace_sens = []\n",
    "    for text in sens:\n",
    "        candidate = []\n",
    "        noun_lis = tokenize(text) #形態素解析\n",
    "        #print(noun_lis)\n",
    "        for n in noun_lis:\n",
    "            nouns = wordnet_get(n) #get related words from wordnet\n",
    "            for noun in nouns:\n",
    "                sample = text.replace(n,noun) #replace\n",
    "                candidate.append(sample) #add candidate\n",
    "        try:\n",
    "            pickup = random.sample(candidate,num) #新しく生成した文章の中からランダムにn個抽出する\n",
    "            replace_sens.extend(pickup)\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            \n",
    "    return replace_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\"tag_decition.csv\",encoding=\"utf-8\",engine=\"python\")  # データの読み込み\\nsentences = df.text.values\\nlabels = df.tag.values\\nX_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2)\\ndf_train = pd.DataFrame({\"text\": X_train,\"tag\":y_train})\\ndf_test  = pd.DataFrame({\"text\": X_test,\"tag\":y_test})\\n#df_train.to_csv(\"train_data.csv\")\\n#df_test.to_csv(\"test_data.csv\")\\n#df\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#この処理は事前処理：訓練データとテストデータ分割保存用処理\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"tag_decition.csv\",encoding=\"utf-8\",engine=\"python\")  # データの読み込み\n",
    "sentences = df.text.values\n",
    "labels = df.tag.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2)\n",
    "df_train = pd.DataFrame({\"text\": X_train,\"tag\":y_train})\n",
    "df_test  = pd.DataFrame({\"text\": X_test,\"tag\":y_test})\n",
    "#df_train.to_csv(\"train_data.csv\")\n",
    "#df_test.to_csv(\"test_data.csv\")\n",
    "#df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_get(name): #.csvまできちっと書いて！！！\n",
    "    #load data\n",
    "    df = pd.read_csv(name,encoding=\"utf-8\",engine=\"python\",usecols=[1,2])\n",
    "    text_lis = []\n",
    "    f_text_lis = []\n",
    "    s_text_lis = []\n",
    "    for text,tag in zip(df[\"text\"],df[\"tag\"]):\n",
    "        if tag == 0:\n",
    "            text_lis.append(text)\n",
    "        elif tag == 1:\n",
    "            f_text_lis.append(text)\n",
    "        else:\n",
    "            s_text_lis.append(text)\n",
    "    \n",
    "    return text_lis,f_text_lis,s_text_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe,flame,spam = tag_get(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 600 136\n"
     ]
    }
   ],
   "source": [
    "print(len(safe),len(flame),len(spam),sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "rep_spam = replace_sens_make(spam,7)\n",
    "rep_spam = random.sample(rep_spam,864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#ここでrep_spamは生成された文章＋元の文章のリストとなっている\n",
    "rep_spam += spam\n",
    "print(len(rep_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "582\n"
     ]
    }
   ],
   "source": [
    "#同様に攻撃的文章に対しても実行する\n",
    "rep_flame = replace_sens_make(flame,1)\n",
    "print(len(rep_flame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_flame = random.sample(rep_flame,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "rep_flame += flame\n",
    "print(len(rep_flame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n",
      "Error\n"
     ]
    }
   ],
   "source": [
    "rep_safe = replace_sens_make(safe,1)\n",
    "rep_safe = random.sample(rep_spam,136)\n",
    "rep_safe += safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(rep_safe),len(rep_flame),len(rep_spam),sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_label = [0]*1000 + [1]*1000 + [2]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rep = rep_safe + rep_flame + rep_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = pd.DataFrame({\"text\": all_rep, \"tag\": rep_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df.to_csv(\"oversample_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
