{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data1/home/yoshida/my_research/data\n"
     ]
    }
   ],
   "source": [
    "%cd ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  9 14:55:40 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 41%   49C    P2   107W / 350W |   6219MiB / 24260MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.10\n",
    "#torch.__version__\n",
    "#torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初回時エラー出るので2回実行する\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\" #Tohoku BERT\n",
    "MODEL_NAME = \"./tapt512_60K/\" #話し言葉コーパス(Katsumata)\n",
    "#MODEL_NAME = \"./hotto-SNS-bert/\" #building of hotto-SNS(cannot exe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for './hotto-SNS-bert/'. Make sure that:\n\n- './hotto-SNS-bert/' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or './hotto-SNS-bert/' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d0f2c5c68ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://huggingface.co/transformers/model_doc/bert.html#<class名>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#bertの学習済みモデルをtokenizerとする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbert_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#学習済みモデルを用いた分類、num_labelで分類する種類の数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gpuに乗せて高速化させるための関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m             )\n\u001b[0;32m-> 1698\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for './hotto-SNS-bert/'. Make sure that:\n\n- './hotto-SNS-bert/' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or './hotto-SNS-bert/' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/model_doc/bert.html#<class名>\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME) #bertの学習済みモデルをtokenizerとする\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3) #学習済みモデルを用いた分類、num_labelで分類する種類の数\n",
    "bert_sc = bert_sc.cuda() #gpuに乗せて高速化させるための関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_auto(name):\n",
    "    #データの読み込み\n",
    "    df = pd.read_csv(name + \".csv\",encoding=\"utf-8\",engine=\"python\",usecols=[1])\n",
    "\n",
    "    with open(\"./all_flaming.txt\",\"r\",encoding=\"UTF-8\") as f:\n",
    "        word_lis = [s.strip() for s in f.readlines()]\n",
    "        \n",
    "    #print(word_lis)\n",
    "    flag = 0\n",
    "    text_lis = []\n",
    "    f_text_lis = []\n",
    "    text_labels = []\n",
    "    f_text_labels = []\n",
    "    for text in df[\"text\"]:\n",
    "        for word in word_lis:\n",
    "            if word in text:\n",
    "                f_text_lis.append(text)\n",
    "                f_text_labels.append(1)\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag :\n",
    "            flag = 0\n",
    "        else:\n",
    "            text_lis.append(text)\n",
    "            text_labels.append(0)\n",
    "        \n",
    "    return text_lis,text_labels, f_text_lis, f_text_labels\n",
    "\n",
    "#plan to make this after getting data from everyone\n",
    "def tag_get(name):\n",
    "    #load data\n",
    "    df = pd.read_csv(name + \".csv\",encoding=\"utf-8\",engine=\"python\",usecols=[1,2])\n",
    "    text_lis = []\n",
    "    f_text_lis = []\n",
    "    s_text_lis = []\n",
    "    for text,tag in zip(df[\"text\"],df[\"tag\"]):\n",
    "        if tag == 0:\n",
    "            text_lis.append(text)\n",
    "        elif tag == 1:\n",
    "            f_text_lis.append(text)\n",
    "        else:\n",
    "            s_text_lis.append(text)\n",
    "    \n",
    "    return text_lis,f_text_lis,s_text_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_list, labels, f_text, f_labels = tag_auto(\"./preprocess\") #危険辞書からラベルづけを自動化\n",
    "text_list, f_text, s_text = tag_get(\"./tag_decition\") #手動でタグ付けしたデータの利用(usecolは別途指定)\n",
    "\n",
    "safety_len = len(text_list)\n",
    "flaming_len = len(f_text)\n",
    "spam_len = len(s_text)\n",
    "labels = [0] * safety_len #tag_get ver\n",
    "f_labels = [1] * flaming_len\n",
    "s_labels = [2] * spam_len\n",
    "print(safety_len,flaming_len,spam_len)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの符号化\n",
    "def Encoding(text_list, labels, max_length=128):\n",
    "    dataset_for_loader = []\n",
    "    for idx,text in enumerate(text_list):\n",
    "        encoding = tokenizer(text, max_length=max_length,padding=\"max_length\",truncation=True) #textを形態素解析、\"pt\"でtensor出力,辞書型でreturn\n",
    "        encoding[\"labels\"] = labels[idx] #add label\n",
    "        encoding = {k: torch.tensor(v) for k,v in encoding.items()}  \n",
    "        dataset_for_loader.append(encoding)\n",
    "    return dataset_for_loader\n",
    "\n",
    "max_length = 128\n",
    "dataset_load_safe = Encoding(text_list,labels,max_length)\n",
    "dataset_load_flaming = Encoding(f_text,f_labels,max_length)\n",
    "dataset_load_spam = Encoding(s_text, s_labels,max_length)  #3値分類に使用、スパム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#確認用\n",
    "print(dataset_load_safe[19])\n",
    "print(\"-\"*60)\n",
    "print(dataset_load_flaming[19])\n",
    "print(\"-\"*60)\n",
    "print(dataset_load_spam[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_list(lis,seed=42):\n",
    "    random.seed()\n",
    "    random.shuffle(lis)\n",
    "    return lis\n",
    "\n",
    "#データセットの分割\n",
    "def dataset_separate(dataset):\n",
    "    n = len(dataset)\n",
    "    n_train = int(0.6*n)\n",
    "    n_val = int(0.2*n)\n",
    "    dataset_train = dataset[:n_train]\n",
    "    dataset_val = dataset[n_train:n_train + n_val]\n",
    "    dataset_test = dataset[n_train+n_val:]\n",
    "    return dataset_train, dataset_val, dataset_test\n",
    "\n",
    "t_dataset_train,t_dataset_val, t_dataset_test = dataset_separate(dataset_load_safe)\n",
    "f_dataset_train,f_dataset_val, f_dataset_test = dataset_separate(dataset_load_flaming)\n",
    "s_dataset_train,s_dataset_val, s_dataset_test = dataset_separate(dataset_load_spam) #spam ver\n",
    "dataset_train = t_dataset_train + f_dataset_train + s_dataset_train\n",
    "dataset_val = t_dataset_val + f_dataset_val + s_dataset_val\n",
    "dataset_test = t_dataset_test + f_dataset_test + s_dataset_test\n",
    "dataset_train = random_list(dataset_train)\n",
    "dataset_val = random_list(dataset_val)\n",
    "dataset_test = random_list(dataset_test)\n",
    "\n",
    "#データセットからデータローダを作成\n",
    "#学習データはshuffle=Trueにする\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        #model_name: name of transformers model\n",
    "        #num_labels: num of labels\n",
    "        #lr: learning rate\n",
    "        \n",
    "        super().__init__()\n",
    "        #num_labels,lrを保存\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    #テストデータのミニバッチが与えられたときテストデータを評価する指標を計算する関数を書く\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log(\"train_loss\", loss,prog_bar=True,logger=True,on_epoch=True,on_step=True) #損失をtrain_lossの名前でログを取る\n",
    "        return loss\n",
    "    \n",
    "    #検証データ版の評価関数\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log(\"val_loss\", val_loss,prog_bar=True,logger=True,on_epoch=True,on_step=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop(\"labels\") #バッチからラベルの取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0) #精度\n",
    "        self.log(\"accuracy\", accuracy)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", mode=\"min\", save_top_k=1,save_weights_only=True, dirpath=\"model/\")\n",
    "\n",
    "#学習方法の指定\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, callbacks = [checkpoint],logger=[pl_loggers.TensorBoardLogger(\"logs/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification_pl(MODEL_NAME, num_labels=3, lr=1e-5)\n",
    "\n",
    "#fine-Tuning\n",
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model_path = checkpoint.best_model_path #file of best model\n",
    "print(\"ベストモデルのファイル: \", checkpoint.best_model_path)\n",
    "print(\"ベストモデルの検証データに対する損失: \", checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard    #sshで6006につなぐと見られる\n",
    "#%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(test_dataloaders=dataloader_test)\n",
    "print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load to pytorch-lightning model\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(best_model_path)\n",
    "\n",
    "#preserve at \"./model_transformers\" involving transformers model\n",
    "model.bert_sc.save_pretrained(\"./model_tapt_epo40_128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sc = BertForSequenceClassification.from_pretrained(\"./model_ex\",num_labels=2)\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
