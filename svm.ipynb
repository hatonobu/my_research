{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data1/home/yoshida/my_research/data\n"
     ]
    }
   ],
   "source": [
    "%cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"test_data\"\n",
    "train_data = \"train_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data1/home/yoshida/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from math import sqrt\n",
    "import gensim.parsing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import word2vec,LsiModel\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import collections\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from func1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "def doc2word_list(text_lis):\n",
    "    tagger = MeCab.Tagger('-Ochasen')\n",
    "    doc_lis = []\n",
    "\n",
    "    for text in text_lis:\n",
    "        node = tagger.parseToNode(text)\n",
    "        word_list = []\n",
    "        while node:\n",
    "            meta = node.feature.split(',')\n",
    "            if meta[0] == '名詞' or meta[0] == \"動詞\" or meta[0] == \"形容詞\" or meta[0] == \"副詞\":\n",
    "                if meta[6] == \"*\":\n",
    "                    word_list.append(node.surface)\n",
    "                else:\n",
    "                    word_list.append(meta[6])\n",
    "            node = node.next\n",
    "        doc_lis.append(word_list)\n",
    "        \n",
    "    return doc_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['チャイナと北鮮に言ってやれ!核を放棄しろと!そして、お前らは非人道的過ぎると!', '科学検察は政治や軍部の下にいると科学的な検察が出来ないから独立性がないといけないんだけども、軍と政治以外で金の出所がない。', '「ここ第一で」、なるほどですね。|それなら思いきって書いちゃうことにしよう。||「オナペット」も昭和の言葉ですね。昭和エロ語辞典ができそうです。', '【極悪国策偏向反中反露:nhk】記録的高温のカナダ西部大規模な山火事発生し町の大部分焼失|★異常気象は利潤原理の資本主義のツケやが、それに扱いしよと思たら、既にそれは社会主義政策になっとるやろ。ダボス会議がグレートリセットと言い出しとるんが何よりの証拠や。', '民主主義社会においては\"社会を構成する人員の最大公約数\"への「最適化」が\"公平\"に一番近付くのに、活動家は自分(達)に合わせた「最適化」を求めるから\"社会の解る\"との間に齟齬が生じるのです|実態としては差別主義思想に基く\"優遇\"を求めているから非難されるのですよ|[定期]']\n"
     ]
    }
   ],
   "source": [
    "def tag_get(name):\n",
    "    #load data\n",
    "    df = pd.read_csv(name + \".csv\",encoding=\"utf-8\",engine=\"python\")\n",
    "    text_lis = []\n",
    "    tag_lis = []\n",
    "    for text,tag in zip(df[\"text\"],df[\"tag\"]): #dfの対象名は随時変更する！\n",
    "        text_lis.append(text)\n",
    "        tag_lis.append(tag)\n",
    "    return text_lis , tag_lis\n",
    "\n",
    "text_lis, tag_lis = tag_get(\"oversample_text\")\n",
    "print(text_lis[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# 名詞のリストになった記事群\n",
    "t_lis = []\n",
    "for text in text_lis:\n",
    "    preprocessed = gensim.parsing.preprocess_string(text)\n",
    "    pre = \"\".join(preprocessed)\n",
    "    #print(pre)\n",
    "    t_lis.append(pre)\n",
    "    \n",
    "#print(t_lis)\n",
    "documents = doc2word_list(t_lis)\n",
    "unfilter = doc2word_list(text_lis)\n",
    "\n",
    "#print(documents)\n",
    "dic = corpora.Dictionary(documents)\n",
    "# 「出現頻度が3未満の単語」と「60%以上の文書で出現する単語」を排除\n",
    "unfiltered = dic.token2id.keys()\n",
    "dic.filter_extremes(no_below = 3, no_above = 0.6)\n",
    "filtered = dic.token2id.keys()\n",
    "filtered_out = set(unfiltered) - set(filtered)\n",
    "bow_corpus = [dic.doc2bow(d) for d in documents]\n",
    "# 辞書の保存(未知文書分類のため)\n",
    "dic.save_as_text('a.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "9797 4345\n",
      "5452\n",
      "[['チャイナ', '北', '鮮', '言う', 'やる', '核', '放棄', 'する', 'お前', 'ら', '人道的', '過ぎる'], ['科学', '検察', '政治', '軍部', '下', 'いる', '科学', '的', '検察', '出来る', '独立', '性', 'ない', 'いける', 'ん', '軍', '政治', '以外', '金', '出所', 'ない'], ['ここ', '一', 'なる', 'それ', '思いきる', '書く', 'ちゃう', 'こと', 'する', 'オナペット', '昭和', '言葉', '昭和', 'エロ', '語', '辞典', 'できる', 'そう']]\n",
      "[(1, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_corpus))\n",
    "print(len(set(unfiltered)), len(set(filtered)))\n",
    "print(len(filtered_out))\n",
    "print(documents[:3])\n",
    "print(bow_corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec2dense(vec, num_terms):\n",
    "    '''Convert from sparse gensim format to dense list of numbers'''\n",
    "    return list(gensim.matutils.corpus2dense([vec], num_terms=num_terms).T[0])\n",
    "\n",
    "bow_docs = {}\n",
    "bow_docs_all_zeros = {}\n",
    "for i,text in enumerate(documents):\n",
    "    sparse = dic.doc2bow(text)\n",
    "    bow_docs[i] = sparse\n",
    "    dense = vec2dense(sparse, num_terms=len(dic))\n",
    "    #print(dense)\n",
    "    bow_docs_all_zeros[i] = all(d == 0 for d in dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "[(1, 1), (13, 1), (23, 1), (46, 1), (151, 1), (193, 1), (210, 1), (235, 1), (262, 1), (611, 1), (654, 1), (722, 1), (770, 1), (883, 1), (1182, 1), (1817, 1), (2177, 1), (2178, 1)]\n"
     ]
    }
   ],
   "source": [
    "#print(bow_docs_all_zeros)\n",
    "print(len(bow_docs))\n",
    "print(bow_docs[503])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSI次元削除,num_topicの次元数に削減する\n",
    "lsi_docs = {}\n",
    "num_topics = 3\n",
    "lsi_model = LsiModel(bow_docs.values(),\n",
    "                           id2word=dic.load_from_text(\"a.txt\"),\n",
    "                           num_topics=num_topics)\n",
    "\n",
    "for i in range(len(bow_docs)):\n",
    "    vec = bow_docs[i]\n",
    "    sparse = lsi_model[vec]\n",
    "    dense = vec2dense(sparse, num_topics)\n",
    "    lsi_docs[i] = sparse\n",
    "    #print(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lsi_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize a vector\n",
    "unit_vecs = {}\n",
    "\n",
    "for i in range(len(lsi_docs)):\n",
    "    vec = vec2dense(lsi_docs[i],num_topics)\n",
    "    norm = sqrt(sum(num**2 for num in vec))\n",
    "    if norm == 0:\n",
    "        norm = 1\n",
    "    unit_vec = [num / norm for num in vec]\n",
    "    unit_vecs[i] = unit_vec\n",
    "\n",
    "#print(unit_vecs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "#text_lis,tag_lis = random_list(text_lis,tag_lis)\n",
    "\n",
    "X_train = vectorizer.fit_transform(text_lis)\n",
    "svd = TruncatedSVD(3)\n",
    "X_train = svd.fit_transform(X_train)\n",
    "\n",
    "# パラメータを dict 型で指定\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],  'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# validation set は GridSearchCV が自動で作成してくれるため，\n",
    "# training set と test set の分割のみを実行すればよい\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "\n",
    "# fit 関数を呼ぶことで交差検証とグリッドサーチがどちらも実行される\n",
    "grid_search.fit(X_train, tag_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [unit_vecs[i] for i in range(len(unit_vecs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# アンダーサンプリング\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, tag_lis, test_size=0.2)\n",
    "target = \"tag\"\n",
    "rs = RandomUnderSampler(random_state=42)\n",
    "under_sampling ,label = rs.fit_resample(X_train, y_train)\n",
    "\n",
    "clf = SVC(C=10,gamma=0.1, kernel=\"rbf\")\n",
    "clf.fit(under_sampling,label)\n",
    "#print(under_sampling)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "pd_label = clf.predict(X_test)\n",
    "print(len(pd_label))\n",
    "#print(pd_label[:300])\n",
    "#print(tag_lis[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictionの準備\n",
    "val_lis, valtag_lis = tag_get(test_data)\n",
    "#print(val_lis[:5])\n",
    "textval_lis = []\n",
    "for text in val_lis:\n",
    "    preprocessed = gensim.parsing.preprocess_string(text)\n",
    "    pre = \"\".join(preprocessed)\n",
    "    #print(pre)\n",
    "    textval_lis.append(pre)\n",
    "    \n",
    "#print(t_lis)\n",
    "docs = doc2word_list(textval_lis)\n",
    "val_corpus = [dic.doc2bow(d) for d in docs]\n",
    "val_docs = {}\n",
    "bow_docs_all_zeros = {}\n",
    "for i,text in enumerate(docs):\n",
    "    sparse = dic.doc2bow(text)\n",
    "    val_docs[i] = sparse\n",
    "    dense = vec2dense(sparse, num_terms=len(dic))\n",
    "    #print(dense)\n",
    "    bow_docs_all_zeros[i] = all(d == 0 for d in dense)\n",
    "    \n",
    "#LSI次元削除,num_topicの次元数に削減する\n",
    "lsi_valdocs = {}\n",
    "num_topics = 3\n",
    "lsi_model = LsiModel(val_docs.values(),\n",
    "                           id2word=dic.load_from_text(\"a.txt\"),\n",
    "                           num_topics=num_topics)\n",
    "\n",
    "for i in range(len(val_docs)):\n",
    "    vec = val_docs[i]\n",
    "    sparse = lsi_model[vec]\n",
    "    dense = vec2dense(sparse, num_topics)\n",
    "    lsi_valdocs[i] = sparse\n",
    "    #print(dense)\n",
    "\n",
    "#Normalize a vector\n",
    "unit_valvecs = {}\n",
    "\n",
    "for i in range(len(lsi_valdocs)):\n",
    "    vec = vec2dense(lsi_valdocs[i],num_topics)\n",
    "    norm = sqrt(sum(num**2 for num in vec))\n",
    "    if norm == 0:\n",
    "        norm = 1\n",
    "    unit_vec = [num / norm for num in vec]\n",
    "    unit_valvecs[i] = unit_vec\n",
    "    \n",
    "val_data = [unit_valvecs[i] for i in range(len(unit_valvecs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.39      0.47       213\n",
      "         1.0       0.50      0.39      0.43       153\n",
      "         2.0       0.13      0.53      0.21        34\n",
      "\n",
      "    accuracy                           0.40       400\n",
      "   macro avg       0.40      0.44      0.37       400\n",
      "weighted avg       0.51      0.40      0.43       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predictionの実行\n",
    "predict_label = clf.predict(val_data)\n",
    "print(classification_report(valtag_lis, predict_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Tf-Idfによる重要度重み付け\\ntfidf_model = models.TfidfModel(bow_docs.values())\\ntfidf_corpus = tfidf_model[bow_corpus]\\n#tf-idfモデルの保存(未知文書分類のため)\\ntfidf_model.save('tfidf_model.model')\\n\\n#LSIによる次元削減\\nlsi_model = models.LsiModel(tfidf_corpus, id2word = dic, num_topics = 3)\\nlsi_corpus = lsi_model[tfidf_corpus]\\n#lsiモデルの保存(未知文書分類のため)\\nlsi_model.save('lsi_model.model')\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF-IDFとLSIによる次元削減による試み\n",
    "\"\"\"\n",
    "#Tf-Idfによる重要度重み付け\n",
    "tfidf_model = models.TfidfModel(bow_docs.values())\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "#tf-idfモデルの保存(未知文書分類のため)\n",
    "tfidf_model.save('tfidf_model.model')\n",
    "\n",
    "#LSIによる次元削減\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word = dic, num_topics = 3)\n",
    "lsi_corpus = lsi_model[tfidf_corpus]\n",
    "#lsiモデルの保存(未知文書分類のため)\n",
    "lsi_model.save('lsi_model.model')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.37      0.36       205\n",
      "           1       0.41      0.51      0.46       174\n",
      "           2       0.58      0.46      0.51       221\n",
      "\n",
      "    accuracy                           0.44       600\n",
      "   macro avg       0.45      0.45      0.44       600\n",
      "weighted avg       0.46      0.44      0.45       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pd_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここからTF-IDFを用いたベクトル化による実験処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_list(x,y,seed=42):\n",
    "    random.seed()\n",
    "    p = list(zip(x, y))\n",
    "    random.shuffle(p)\n",
    "    x, y = zip(*p)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'gamma': [0.001, 0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lis, tag_lis = tag_get(\"oversample_text\")\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "#text_lis,tag_lis = random_list(text_lis,tag_lis)\n",
    "\n",
    "X_train = vectorizer.fit_transform(text_lis)\n",
    "svd = TruncatedSVD(3)\n",
    "X_train = svd.fit_transform(X_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# パラメータを dict 型で指定\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],  'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# validation set は GridSearchCV が自動で作成してくれるため，\n",
    "# training set と test set の分割のみを実行すればよい\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "\n",
    "# fit 関数を呼ぶことで交差検証とグリッドサーチがどちらも実行される\n",
    "grid_search.fit(X_train, tag_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1000, 1: 1000, 2: 1000})\n"
     ]
    }
   ],
   "source": [
    "l = collections.Counter(tag_lis)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2\n",
      " 0 2 2 2 2 0 2 2 2 2 2 2 2 0 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 1 0 2 2\n",
      " 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 1\n",
      " 2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 0 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 2 2 2 0 2 1 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 0 2 0 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Test set score: 0.12\n",
      "Best parameters: {'C': 100, 'gamma': 100}\n",
      "Best cross-validation: 0.5366666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.68      0.06      0.11       213\n",
      "         1.0       0.38      0.02      0.04       153\n",
      "         2.0       0.09      0.94      0.16        34\n",
      "\n",
      "    accuracy                           0.12       400\n",
      "   macro avg       0.38      0.34      0.10       400\n",
      "weighted avg       0.52      0.12      0.09       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = tag_get(\"test_data\")\n",
    "X_test = vectorizer.fit_transform(X_test)\n",
    "svd = TruncatedSVD(3)\n",
    "X_test = svd.fit_transform(X_test)\n",
    "\n",
    "pd_label = grid_search.predict(X_test)\n",
    "print(pd_label)\n",
    "\n",
    "print('Test set score: {}'.format(grid_search.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best cross-validation: {}'.format(grid_search.best_score_))\n",
    "print(classification_report(y_test,pd_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
